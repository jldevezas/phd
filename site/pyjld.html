<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<title>Community-Driven Music Discovery</title>

		<meta name="description" content="">
		<meta name="author" content="phd, visualization, devezas, music, discovery, communities, social">
		<meta name="HandheldFriendly" content="True">
		<meta name="MobileOptimized" content="320">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
		
		<link rel="stylesheet" type="text/css" href="css/ink-min.css">
		<link rel="stylesheet" type="text/css" href="css/select2.css">
		<link rel="stylesheet" type="text/css" href="css/highlight/zenburn.css">
		<link rel="stylesheet" type="text/css" href="css/jld.css">
		<!--[if lte IE 7 ]>
		<link rel="stylesheet" href="css/ink-ie-min.css" type="text/css" media="screen"
		title="no title" charset="utf-8">
		<![endif]-->

		<script type="text/x-mathjax-config" charset="utf-8">
			MathJax.Hub.Config({
				tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] },
			});
		</script>
		<script type="text/javascript" charset="utf-8"
			src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>

		<script src="js/jquery-1.9.1.min.js" type="text/javascript" charset="utf-8"></script>
		<script type="text/javascript" charset="utf-8">
			$(document).on("ready", function() {
				$("#top").load("top.html");
				$("#bottom").load("bottom.html");
			});
		</script>
	</head>

	<body>
		<div id="top"></div>

		<div class="ink-grid">
			<div class="column-group gutters top-space">
				<div class="large-50">
					<h4>PyJLD: A workbench for latent factor models</h3>
					<p>
						As part of the PhD work, we've developed an open source Python library to experiment with
						latent factor models. We've implemented SVD-based rating prediction and recommendation, and
						a k-fold cross-validation method for the evaluation of the model and to compare the SVD
						baseline to any further improvements we plan to add based on the social dimension.
						Additionally, we've also extended the cross-validation method to enable the identification
						of the number of latent factors that minimizes the mean absolute error.
					</p>

					<p>
						Download PyJLD from <a
							href="https://github.com/jldevezas/phd/tree/master/recommender-systems/matrix-factorization">GitHub</a>
						by cloning my PhD repository. You can find the library and the CLI tools under
						<code>recommender-systems/matrix-factorization</code>.
						
						<div class="box">
							<code>git clone git@github.com:jldevezas/phd.git</code>
						</div>
					</p>

					<p>
						You can also find an <a
							href="https://github.com/jldevezas/phd/blob/master/recommender-systems/r-workspace/evaluation-charts.R">R
							ggplot2 script</a> under <code>recommender-systems/r-workspace</code>, named
						<code>evaluation-charts.R</code> to make a cool looking chart like the one on the right.
						That script can directly be used with the CVS generated by <code>cross_validation.py</code>
						with the <code>--output</code> option when using the
						<code>--feature-sampling-interval</code> argument.
					</p>
				</div>
				<div class="large-50">
					<div class="vertical-space">
						<img src="img/feature_size_mae.png"/>
					</div>
					<p class="small">
						<b>Figure 1:</b> Average, for 10 folds, of the mean absolute error, over a variable number
						of latent factors, sampled in a logarithmic manner. Thus, the sampling frequency was higher
						for lower numbers of latent factors, since the error tends to stabilize for larger numbers
						of features. The ideal number of features is highlighted and represents the minimum error
						value.
					</p>
				</div>

				<div class="large-100">
					<h3>Manual</h3>

					<p>PyJLD consists of the library <code>jld.py</code> and the following command line tools that
						implement and serve as an example of the methods in this library: <code>train.py</code>,
						<code>predict.py</code>, <code>recommend.py</code>, <code>recommend_by_query.py</code>,
						<code>nn.py</code>, <code>cross_validation.py</code> and <code>server.py</code>.</p>

					<h4><code>jld.py</code></h4>

					<p>
						This library will comprise all the Python classes that I develop during my doctoral project.
						For now, however, it only contains a single class named <code>LatentFactorsModel</code>
						that implements rating prediction and recommendation using Singular Value Decomposition
						(SVD), as well as k-fold cross-validation to evaluate the model and compare it to any
						future implementations as well as to find the number of latent factors that minimizes the
						error.
					</p>

					<p>
						Available methods for <code>LatentFactorsModel</code> are listed below, along with an
						explanation of how to use them:
					</p>

					<div class="box">
						<dl>
							<dt><code>LatentFactorsModel(h5filename)</code></dt>
							<dd>
								Creates a new latent factors model class that enables you to train, predict,
								recommend and validate your model, which is stored completely in disk using the
								HDF5 format in the provided <code>h5filename</code> file path.
							</dd>

							<dt><code>enable_normalization()</code></dt>
							<dd>
								Turns on ratings normalization for the user vector, scaling values from zero to one.
							</dd>

							<dt><code>disable_normalization()</code></dt>
							<dd>
								All ratings are used exactly as they are given.
							</dd>
							
							<dt><code>set_training_rank(rank)</code></dt>
							<dd>
								Currently unused, but will be useful to limit number of latent factors and for
								non-negative matrix factorization, to be offered as an option later on.
							</dd>

							<dt><code>set_training_sample_size(sample_size)</code></dt>
							<dd>
								Limit the number of ratings (lines) to be read from the training set CSV file.
							</dd>

							<dt><code>set_training_csv_delimiter(delimiter)</code></dt>
							<dd>
								Set CVS delimiter to something other than the comma character.
							</dd>

							<dt><code>get_training_rank()</code></dt>
							<dd>
								Return the currently set rank value.
							</dd>

							<dt><code>get_training_sample_size()</code></dt>
							<dd>
								Return the currently set training set sample size, corresponding to the number of
								lines read from the training data CSV.
							</dd>

							<dt><code>get_training_csv_delimiter()</code></dt>
							<dd>
								Return the currently set CSV delimiter character, which by default is a comma.
							</dd>

							<dt><code>train(cvs_path)</code></dt>
							<dd>
								Load CSV data in the format <code>(user,item,rating)</code> into a bidimensional
								HDF5 dataset, as a user-item matrix; factorize this matrix supported on disk
								storage and using singular value decomposition; and store the resulting matrices.
								For more details on how SVD works, please read the hopefully quite clear <a
									href="latent-factor-models.html">SVD for dummies in the context of recommender
									systems</a>.
							</dd>

							<dt><code>predict(user_id, item_id)</code></dt>
							<dd>
								Given a <code>user_id</code> and an <code>item_id</code> in the same format as the
								original training data CSV, return the, possibly precomputed, predicted rating.
								Note that this method always returns the predicted rating, even if the user had
								specifically rated the item in the training set.
							</dd>

							<dt><code>get_rating(user_id, item_id)</code></dt>
							<dd>
								Given a <code>user_id</code> and an <code>item_id</code> in the same format as the
								original training data CSV, return the original rating or <code>None</code> if the
								user didn't rate the specified item or if other error occurred, in which case a
								message will be logged to the command line, with the error description.
							</dd>

							<dt><code>precompute_predictions()</code></dt>
							<dd>
								Assuming that the training has been done and is stored in the currently defined
								HDF5 model file, calculate and store the rating predictions for each user.
							</dd>

							<dt><code>recommend(user_id, limit=None, all=False)</code></dt>
							<dd>
								Given a <code>user_id</code> in the same format as the original training data CSV,
								return a ranked list of items according to their predicted ratings for the user. If
								<code>limit</code> is set, then the result will be only the top <code>limit</code>
								recommendations. If <code>all</code> is set, then all items, including those
								specifically rated by the user will also be included in the results, ranked
								according to the predicted rating.
							</dd>

							<dt><code>recommend_by_query(item_ratings, all=False)</code></dt>
							<dd>
								Given a user vector of <code>item_ratings</code>, which must be the same dimension
								as the number of items available in the model, return an iterator of items ranked
								according to the predicted rating. This is useful if you want to train a model and
								then query it later for new users of a system.
							</dd>

							<dt><code>nearest_neighbors(item_ratings, distance=scipy.spatial.distance.euclidean,
								limit=5)</code></dt>
							<dd>
								Given a user vector of <code>item_ratings</code>, which must be the same dimension
								as the number of items available in the model, return the top <code>limit</code>
								nearest neighbors according to the distance metric provided, which defaults to the
								euclidean distance.
							</dd>

							<dt><code>nearest_neighbor(self, item_ratings,
								distance=scipy.spatial.distance.euclidean)</code></dt>
							<dd>
								Given a user vector of <code>item_ratings</code>, which must be the same dimension
								as the number of items available in the model, return the nearest neighbor,
								according to the distance metric provided, which defaults to the euclidean
								distance. This is equivalent to <code>nearest_neighbors(item_ratings, distance,
									limit=1)</code>.
							</dd>

							<dt><code>mean_absolute_error(original_prediction_indices_tuples)</code></dt>
							<dd>
								Given a tuple <code>original_prediction_indices_tuples</code> containing the
								original ratings vector, the predicted ratings vector and the indices of the held
								ratings, return the mean absolute error. You can read more about evaluation in the
								<a href="k-fold-cross-validation.html">k-fold cross-validation for dummies</a>
								section where you can also find the formula for this metric.
							</dd>

							<dt><code>root_mean_squared_error(original_prediction_indices_tuples)</code></dt>
							<dd>
								Given a tuple <code>original_prediction_indices_tuples</code> containing the
								original ratings vector, the predicted ratings vector and the indices of the held
								ratings, return the root mean squared error.  You can read more about evaluation in
								the <a href="k-fold-cross-validation.html">k-fold cross-validation for dummies</a>
								section where you can also find the formula for this metric.
							</dd>

							<dt><code>k_fold_cross_validation(original_csv_path, k=10, given_fraction=0.8,
								feature_sampling=None, max_features=None, output_filename=None)</code></dt>
							<dd>
								Given the <code>original_csv_path</code> for the training set CSV, the number
								<code>k</code> of folds and the <code>given_fraction</code> of ratings to use in
								the prediction, calculate and print the mean absolute error and the root mean
								squared error of the SVD recommendation algorithm. If <code>feature_sampling</code>
								is set (e.g. <code>n=50</code>), then the system is validated for an interval of
								features varying from zero to the maximum possible value, in a logarithmic interval
								for <code>n</code> different numbers of latent factors. Since the erro tends to
								converge for large numbers of features, you can also use <code>max_features</code>
								to discard latent factors over that value. Validation results can be saved in a
								CVS given by <code>output_filename</code>.
							</dd>
						</dl>
					</div>

					<h4><code>train.py</code></h4>

					<div class="box">
<pre>
<b>usage: train.py [-h] [-d DELIMITER] [-r RANK] [-s SIZE] [--precompute]
				ratings_path model_path</b>

<em>Train model based on SVD matrix factorization for user-item rating prediction.</em>

positional arguments:
  <b>ratings_path</b>          a CSV file with no header and three columns: user_id,
                        item_id, rating number
  <b>model_path</b>            HDF5 file to store the trained model containing the
                        factorized matrices

optional arguments:
  <b>-h, --help</b>            show this help message and exit
  <b>-d DELIMITER, --delimiter DELIMITER</b>
                        the CSV column delimiter character (DEFAULT=',')
  <b>-r RANK, --rank RANK</b>  the number of latent factors (DEFAULT=1000)
  <b>-s SIZE, --size SIZE</b>  the size of the sample to take from the ratings CSV
                        (DEFAULT=None)
  <b>--precompute</b>          precompute and store prediction values in HDF5
                        (DEFAULT=False)
</pre>
					</div>
					

					<h4><code>predict.py</code></h4>

					<div class="box">
<pre>
<b>usage: predict.py [-h] [-f] user item model_path</b>

<i>Predict a normalized rating for any unrated item of an existing user.</i>

positional arguments:
  <b>user</b>                 user identification string, as defined in the training
                       set
  <b>item</b>                 item identification string, as defined in the training
                       set
  <b>model_path</b>           HDF5 file with the trained model containing the
                       factorized matrices

optional arguments:
  <b>-h, --help</b>           show this help message and exit
  <b>-f, --force-predict</b>  returns the predicted value even when the original
                       rating is available
</pre>
					</div>

					<h4><code>recommend.py</code></h4>

					<div class="box">
<pre>
<b>usage: recommend.py [-h] user model_path</b>

<i>Recommend new items to an existing user ordered by predicted rating.</i>

positional arguments:
  <b>user</b>        user identification string, as defined in the training set
  <b>model_path</b>  HDF5 file with the trained model containing the factorized
              matrices

optional arguments:
  <b>-h, --help</b>  show this help message and exit
</pre>
					</div>

					<h4><code>recommend_by_query.py</code></h4>

					<div class="box">
<pre>
<b>usage: recommend_by_query.py [-h] [--love LOVE] [--like LIKE]
                             [--neutral NEUTRAL] [--dislike DISLIKE]
                             [--hate HATE]
                             model_path</b>

<i>Use qualitative users preferences to recommend new items.</i>

positional arguments:
  <b>model_path</b>         HDF5 file with the trained model containing the
                     factorized matrices

optional arguments:
  <b>-h, --help</b>         show this help message and exit
  <b>--love LOVE</b>        comma-separated item IDs for loved items
  <b>--like LIKE</b>        comma-separated item IDs for liked items
  <b>--neutral NEUTRAL</b>  comma-separated item IDs for neutral items
  <b>--dislike DISLIKE</b>  comma-separated item IDs for disliked items
  <b>--hate HATE</b>        comma-separated item IDs for hated items
</pre>
					</div>

					<h4><code>nn.py</code></h4>

					<div class="box">
<pre>
<b>usage: nn.py [-h] [--love LOVE] [--like LIKE] [--neutral NEUTRAL]
             [--dislike DISLIKE] [--hate HATE]
			 model_path</b>

<i>Discover nearest neighbor in trained model based on qualitative user input.</i>

positional arguments:
  <b>model_path</b>         HDF5 file with the trained model containing the
                     factorized matrices

optional arguments:
  <b>-h, --help</b>         show this help message and exit
  <b>--love LOVE</b>        comma-separated item IDs for loved items
  <b>--like LIKE</b>        comma-separated item IDs for liked items
  <b>--neutral NEUTRAL</b>  comma-separated item IDs for neutral items
  <b>--dislike DISLIKE</b>  comma-separated item IDs for disliked items
  <b>--hate HATE</b>        comma-separated item IDs for hated items
</pre>
					</div>

					<h4><code>cross_validation.py</code></h4>

					<div class="box">
<pre>
<b>usage: cross_validation.py [-h] [-d DELIMITER] [-r RANK] [-k FOLDS]
                           [-n FEATURE_SAMPLING_INTERVAL] [-m MAX_FEATURES]
                           [-o OUTPUT]
						   ratings_path</b>

<i>Do k-fold cross-validation by creating k training CSVs from the original.</i>

positional arguments:
  <b>ratings_path</b>          a CSV file with no header and three columns: user_id,
                        item_id, rating number

optional arguments:
  <b>-h, --help</b>            show this help message and exit
  <b>-d DELIMITER, --delimiter DELIMITER</b>
                        the CSV column delimiter character (DEFAULT=',')
  <b>-r RANK, --rank RANK</b>  the number of latent factors (DEFAULT=1000)
  <b>-k FOLDS, --folds FOLDS</b>
                        the number of folds to use in cross-validation
                        (DEFAULT=10)
  <b>-n FEATURE_SAMPLING_INTERVAL, --feature-sampling-interval FEATURE_SAMPLING_INTERVAL</b>
                        the sampling interval in a log space of the number of
                        features to use in cross-validation
  <b>-m MAX_FEATURES, --max-features MAX_FEATURES</b>
                        the maximum number of features to use in cross-
                        validation (ignored if sampling size is not defined)
  <b>-o OUTPUT, --output OUTPUT</b>
                        output CSV filename, to store validation scores (MAE)
</pre>
					</div>

					<h4><code>server.py</code></h4>

					<div class="box">
<pre>
$ python server.py model_path
</pre>
					<p>
						A minimalistic example of how to provide PyJLD recommendation as a Flask service.
					</p>
					</div>

				</div>
			</div>
		</div>

		<div id="bottom"></div>
	</body>
</html>
